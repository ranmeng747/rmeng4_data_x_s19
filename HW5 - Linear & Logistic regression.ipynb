{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-X Spring 2019: Homework 05\n",
    "\n",
    "### Linear regression & Logistic regression\n",
    "\n",
    "\n",
    "\n",
    "## Name: Ran Meng\n",
    "\n",
    "## SID:  3034368718\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this homework, you will do some exercises on prediction using sklearn. \n",
    "\n",
    "REMEMBER TO DISPLAY ALL OUTPUTS. If the question asks you to do something, make sure to print your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data:\n",
    "__Data Source__:\n",
    "Data file is uploaded to bCourses and is named: __Energy.csv__ \n",
    "(Link in the Assignment details page on Bcourses) \n",
    "\n",
    "The dataset was created by Angeliki Xifara ( Civil/Structural Engineer) and was processed by Athanasios Tsanas, Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK).\n",
    "\n",
    "__Data Description__:\n",
    "\n",
    "The dataset contains eight attributes of a building (or features, denoted by X1...X8) and response being the heating load on the building, y1. \n",
    "\n",
    "* X1\tRelative Compactness \n",
    "* X2\tSurface Area \n",
    "* X3\tWall Area \n",
    "*  X4\tRoof Area \n",
    "*  X5\tOverall Height \n",
    "* X6\tOrientation \n",
    "*  X7\tGlazing Area \n",
    "*  X8\tGlazing Area Distribution \n",
    "*  y1\tHeating Load \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1\n",
    "Read the data file from the csv.\n",
    "\n",
    "Print the count of NaN values for each attribute in the dataset.\n",
    "\n",
    "Print the Range (min, max) and percentiles (25th, 50th, and 75th) of each attribute in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of NaN values for each attribute: \n",
      " X1    0\n",
      "X2    0\n",
      "X3    0\n",
      "X4    0\n",
      "X5    0\n",
      "X6    0\n",
      "X7    0\n",
      "X8    0\n",
      "Y1    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.6200</td>\n",
       "      <td>514.500</td>\n",
       "      <td>245.0</td>\n",
       "      <td>110.250</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.9800</td>\n",
       "      <td>808.500</td>\n",
       "      <td>416.5</td>\n",
       "      <td>220.500</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.00</td>\n",
       "      <td>43.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.6825</td>\n",
       "      <td>606.375</td>\n",
       "      <td>294.0</td>\n",
       "      <td>140.875</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>12.9925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.7500</td>\n",
       "      <td>673.750</td>\n",
       "      <td>318.5</td>\n",
       "      <td>183.750</td>\n",
       "      <td>5.25</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>18.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.8300</td>\n",
       "      <td>741.125</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.500</td>\n",
       "      <td>7.00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.00</td>\n",
       "      <td>31.6675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1       X2     X3       X4    X5    X6    X7    X8       Y1\n",
       "min  0.6200  514.500  245.0  110.250  3.50  2.00  0.00  0.00   6.0100\n",
       "max  0.9800  808.500  416.5  220.500  7.00  5.00  0.40  5.00  43.1000\n",
       "25%  0.6825  606.375  294.0  140.875  3.50  2.75  0.10  1.75  12.9925\n",
       "50%  0.7500  673.750  318.5  183.750  5.25  3.50  0.25  3.00  18.9500\n",
       "75%  0.8300  741.125  343.0  220.500  7.00  4.25  0.40  4.00  31.6675"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "df = pd.read_csv('energy.csv')\n",
    "print(\"Count of NaN values for each attribute: \\n\", df.isnull().sum())\n",
    "df.describe().loc[['min','max','25%', '50%', '75%']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __REGRESSION__:\n",
    "\n",
    "Using the data, we want to predict \"Heating load\". The output variable is continuous. Hence, we need to use a regression algorithm.  \n",
    "\n",
    "__Q 1.2:__ \n",
    "\n",
    "Split the dataset randomly into train and test. Train a **Linear Regression** model on 80% of the data (80-20 split).\n",
    "What is the intercept and coefficient values?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  614\n",
      "Number of testing samples:  154\n",
      "\n",
      "Intercept Value:  79.13116174147392\n",
      "Coefficient Values:  [-6.33926290e+01 -5.86380428e-02  3.46024305e-02 -4.66202367e-02\n",
      "  4.36194652e+00  1.81224259e-02  1.98760201e+01  2.19167208e-01]\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.loc[:, 'X1':'X8']\n",
    "Y = df[\"Y1\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 100)\n",
    "print('Number of training samples: ', len(X_train))\n",
    "print('Number of testing samples: ', len(X_test))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "print('\\nIntercept Value: ', model.intercept_)\n",
    "print('Coefficient Values: ', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### Q.1.3: \n",
    "\n",
    "Create a function which takes arrays of prediction and actual values of the output as parameters to calculate **'Root Mean Square error'** (RMSE) metric:  \n",
    "\n",
    "1. Use the function to calculate the training RMSE  \n",
    "2. Use the function to calculate the test RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE:  2.9242420751260125\n",
      "Testing RMSE:  2.905413624299769\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "def RMSE(prediction, actual):\n",
    "    return np.sqrt(np.mean(np.square(prediction - actual)))\n",
    "\n",
    "# Calculate RMSE for training data:\n",
    "Y_pred_train = model.predict(X_train)\n",
    "print ('Training RMSE: ', RMSE(Y_pred_train, Y_train))\n",
    "\n",
    "# Calculate RMSE for testing data:\n",
    "Y_pred_test = model.predict(X_test)\n",
    "print ('Testing RMSE: ', RMSE(Y_pred_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q1.4: \n",
    "\n",
    "Let's see the effect of amount of data on the performance of prediction model. Use varying amounts of data (100,200,300,400,500,all) from the training data you used previously to train different regression models. Report  training error and test error in each case. Test data  is the same as above for  all  these cases.\n",
    "\n",
    "**Plot error rates vs number of training examples.** Both the training error and the test error should be plotted. Comment on the relationship you observe between the amount of data used to train the model and the test accuracy of the model.\n",
    "\n",
    "__Hint:__ Use array indexing to choose varying data amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy for 100 training samples: 0.9331810075948953\n",
      "\n",
      "Testing Accuracy for 100 training samples: 0.9043716390104454\n",
      "\n",
      "Training Accuracy for 200 training samples: 0.9235558691685484\n",
      "\n",
      "Testing Accuracy for 200 training samples: 0.9026649121371368\n",
      "\n",
      "Training Accuracy for 300 training samples: 0.9210616105122591\n",
      "\n",
      "Testing Accuracy for 300 training samples: 0.9084474728838293\n",
      "\n",
      "Training Accuracy for 400 training samples: 0.9179117938919141\n",
      "\n",
      "Testing Accuracy for 400 training samples: 0.9094167596539908\n",
      "\n",
      "Training Accuracy for 500 training samples: 0.9155288000484815\n",
      "\n",
      "Testing Accuracy for 500 training samples: 0.9080690273465932\n",
      "\n",
      "Training Accuracy for 768 training samples: 0.916183057742754\n",
      "\n",
      "Testing Accuracy for 768 training samples: 0.9103070158682715\n"
     ]
    }
   ],
   "source": [
    "training_size = np.array([100, 200, 300, 400, 500, len(df)])\n",
    "r = training_size/len(df) # training data ratio\n",
    "training_errors = []\n",
    "testing_errors = []\n",
    "\n",
    "for i in range(len(training_size)):\n",
    "    X_train_loop, X_test_loop, Y_train_loop, Y_test_loop = train_test_split(X, Y, test_size = 1- r[i], random_state = 100) \n",
    "    model= LinearRegression()\n",
    "    model.fit(X_train_loop, Y_train_loop)\n",
    "\n",
    "    training_accuracy = model.score(X_train_loop,Y_train_loop)\n",
    "    print ('\\nTraining Accuracy for', training_size[i],  'training samples:', training_accuracy)\n",
    "    testing_accuracy = model.score(X_test,Y_test)\n",
    "    print('\\nTesting Accuracy for',  training_size[i],  'training samples:', testing_accuracy)\n",
    "    \n",
    "    training_errors.append(1- training_accuracy)\n",
    "    testing_errors.append(1- testing_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training_accuracies = np.array([training_accuracy_100, training_accuracy_200,training_accuracy_300,\\\n",
    "#                              training_accuracy_400, training_accuracy_500, training_accuracy_all])\n",
    "#training_error_rates = 1 - training_accuracies\n",
    "\n",
    "#testing_accuracies = np.array([testing_accuracy_100, testing_accuracy_200,testing_accuracy_300,\\\n",
    " #                              testing_accuracy_400, testing_accuracy_500, testing_accuracy_all])\n",
    "#testing_error_rates = 1 - testing_accuracies\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "l1, = ax.plot(training_size, training_errors, color='green', linestyle=':')\n",
    "l2, = ax.plot(training_size, testing_errors, color='orange', linestyle='--')\n",
    "ax.legend(handles=[l1, l2], labels=['Training Error Rates', 'Testing Error Rates'],\n",
    "              loc=[0.55, 0.2],frameon = True,numpoints = 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "**Q1.4 :**\n",
    "I got the definition of the \"error rate\" as \"1- accuracy\" from 4c sklearn-classification-regression-bkhw-Spring 2019-.pdf on Data X Github. It is worth noting that the error rate may also refer to RMSE. \n",
    "\n",
    "To interpret the graph: As we increase the number of training data, we observe a decreasing error rate for test data. This indicates that the linear model is learning the data better with increased amount of training data and no overfitting has occurred. \n",
    "\n",
    "However, the error rate for training data is not monotonic. When the training size is small, there is less data to fit which could result in lower error rate. As the training size gets somewhat large, there is more data to fit and the training error rate could increase. \n",
    "\n",
    "A desired model should have the same error rate for training samples and testing samples, and the smaller the error rate the better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__CLASSIFICATION__:\n",
    "LABELS ARE DISCRETE VALUES.\n",
    "\n",
    "Here the model is trained to classify each instance into a set of predefined discrete classes. On inputting a feature vector into the model, the trained model is able to predict a class of that instance.\n",
    "\n",
    "\n",
    "#### Q2.1\n",
    "Bucket the values of 'y1' i.e 'Heating Load'  from the original dataset into 3 classes:\n",
    "\n",
    "0: 'Low' ( < 14),   \n",
    "1: 'Medium'  (14-28),   \n",
    "2: 'High'  (>28)\n",
    "\n",
    "**HINT:** Use pandas.cut\n",
    "\n",
    "This converts the given dataset  into a classification problem. Use this dataset with transformed 'heating load' to create a **logistic regression** classifiction model that predicts heating load type of a building. Split the data randomly into training and test set. Train the model on 80% of the data (80-20 split).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X1     X2     X3      X4   X5  X6   X7  X8      Y1\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  Medium\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  Medium\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  Medium\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  Medium\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  Medium\n",
      "\n",
      "Number of training samples:  614\n",
      "Number of test samples:  154\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "df2 = df.copy()\n",
    "df2['Y1'] = pd.cut(df2['Y1'], [-np.inf,14,28,np.inf], labels = ['Low', 'Medium', 'High'])\n",
    "print(df2.head())\n",
    "\n",
    "# For the purpose of generating the confusion matrix later, I define label 'Low' with 0, 'Medium' with 1, 'High' with 2\n",
    "X2 = df2.loc[:, 'X1':'X8']\n",
    "Y2 = df2[\"Y1\"]\n",
    "Y2 = Y2.map({'Low': 0, 'Medium': 1,'High' :2})\n",
    "x_train, x_test, y_train, y_test = train_test_split(X2, Y2, test_size = 0.2, random_state = 100)\n",
    "print('\\nNumber of training samples: ', len(x_train))\n",
    "print('Number of test samples: ', len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.2 \n",
    "- Print the training and test accuracies\n",
    "- Print the confusion matrix\n",
    "- Print the precision and recall numbers for all the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8078175895765473\n",
      "Testing Accuracy:  0.7727272727272727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logistic_Model = LogisticRegression()\n",
    "\n",
    "#Training the model and calculating accuracies\n",
    "Logistic_Model.fit(x_train, y_train);\n",
    "\n",
    "training_accuracy = Logistic_Model.score(x_train, y_train)\n",
    "print ('Training Accuracy:', training_accuracy)\n",
    "\n",
    "test_accuracy = Logistic_Model.score(x_test, y_test)\n",
    "print('Testing Accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of test data is: \n",
      "                Predicted Low  Predicted Medium  Predicted High\n",
      "Actual Low                42                 1               0\n",
      "Actual Medium              9                22              25\n",
      "Actual High                0                 0              55\n"
     ]
    }
   ],
   "source": [
    "#Generating the confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = y_test\n",
    "y_pred = Logistic_Model.predict(x_test)\n",
    "CM = pd.DataFrame(confusion_matrix(y_true, y_pred),columns = ['Predicted Low','Predicted Medium','Predicted High'],\\\n",
    "                  index = ['Actual Low','Actual Medium','Actual High']\n",
    "                 )\n",
    "print ('Confusion matrix of test data is: \\n', CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision for the 3 classes is:  [0.82352941 0.95652174 0.6875    ]\n",
      "Average recall for the 3 classes is:  [0.97674419 0.39285714 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print(\"Average precision for the 3 classes is: \", precision_score(y_true, y_pred, average = None) )\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Average recall for the 3 classes is: \", recall_score(y_true, y_pred, average = None) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.3\n",
    "##### K Fold Cross Validation\n",
    "\n",
    " In k-fold cross-validation, the shuffled training data is partitioned into k disjoint sets and the model is trained on k âˆ’1 sets and validated on the kth set. This process is repeated k times with each set chosen as the validation set once. The cross-validation accuracy is reported as the average accuracy of the k iterations\n",
    " \n",
    "__Use 7-fold cross validation on the training data. Print the average accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=7, random_state=None, shuffle=False)\n",
      "\n",
      "Averaged cross validation accuracy:  0.780116435288849\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kf = KFold(n_splits = 7)\n",
    "print(kf)\n",
    "\n",
    "acc = cross_val_score(Logistic_Model, x_train, y_train, cv = kf)\n",
    "\n",
    "avg_acc = np.mean(acc)\n",
    "print(\"\\nAveraged cross validation accuracy: \", avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q2.4__\n",
    "\n",
    "One of the preprocessing steps in Data science is Feature Scaling i.e getting all our data on the same scale by setting same  Min-Max of feature values. \n",
    "This makes training less sensitive to the scale of features . \n",
    "Scaling is important in algorithms that use distance functions as a part of classification. If we Scale features in the range [0,1] it is called unity based normalization.\n",
    "\n",
    "__Perform unity based normalization on the above dataset and train the model again, compare model performance in training and validation with your previous model.__  \n",
    "\n",
    "refer:http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler  \n",
    "more at: https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy before normalization:  0.8078175895765473\n",
      "Training Accuracy after normalization:  0.8224755700325733\n",
      "\n",
      "Testing Accuracy before normalization :  0.7727272727272727\n",
      "Testing Accuracy after normalization :  0.8116883116883117\n"
     ]
    }
   ],
   "source": [
    "X_Norm = (X2-X2.min())/(X2.max()-X2.min())\n",
    "x_train_norm, x_test_norm, y_train, y_test = train_test_split(X_Norm, Y2, test_size = 0.2, random_state = 100)\n",
    "\n",
    "\n",
    "#Training the model and calculating accuracies\n",
    "logistic_model_norm = LogisticRegression()\n",
    "logistic_model_norm.fit(x_train_norm, y_train);\n",
    "\n",
    "training_accuracy_norm = logistic_model_norm.score(x_train_norm, y_train)\n",
    "print ('Training Accuracy before normalization: ', training_accuracy)\n",
    "print ('Training Accuracy after normalization: ', training_accuracy_norm)\n",
    "\n",
    "\n",
    "test_accuracy_norm = logistic_model_norm.score(x_test_norm, y_test)\n",
    "print('\\nTesting Accuracy before normalization : ', test_accuracy)\n",
    "print('Testing Accuracy after normalization : ', test_accuracy_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "**Q2.4 :**\n",
    "By utilizing unity based normalization, both the training accuracy (0.822 > 0.808) and testing accuracy (0.812 > 0.773) have improved compared to using the original dataset. This indicates that the training has become less sensitivity to the scale of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "data-x"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
